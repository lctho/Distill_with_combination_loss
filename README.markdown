# **Text Classification with Knowledge Distillation**

This repository implements a text classification model using the `uitnlp/visobert` Transformer with knowledge distillation, combining cross-entropy (CE) and Kullback-Leibler (KL) divergence loss. The project processes Vietnamese text data for tasks such as intent classification (e.g., ordering, shipping).

## **Data Structure**

### **Training Dataset**

The training dataset is stored in a CSV file (e.g., `trainfile1.csv`) with the following columns:

- **content**: The input text (e.g., `"đặt trà sữa"` (Place an order for milk tea)).
  - Type: String
  - Description: Raw Vietnamese text to be classified.
- **label_id**: The hard label ID.
  - Type: Integer (0 to 13)
  - Description: Maps to a specific class (e.g., `0` for `"ordering"`, `1` for `"shipping"`) as defined in `data_label1.json`.
- **soft_labels**: The soft label probabilities.
  - Type: List of floats (length 14)
  - Description: Probability distribution over classes, generated by a teacher model (ChatGPT in this project). The highest probability aligns with `label_id` (e.g., `[0.8, 0.05, 0.03, ..., 0.01]` where index 0 is highest for `label_id=0`).

**Example Row**:

```
content: "đặt trà sữa" (Place an order for milk tea), label_id: 0, soft_labels: [0.8, 0.05, 0.03, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]
```

#### **Soft Labels Generation**

The `soft_labels` are generated using ChatGPT, which was trained with few-shot learning to classify customer statements in the sales and service domain. Few-shot learning involves providing ChatGPT with a small number of labeled examples (prompts) to guide its understanding of the classification task. The prompt used to train ChatGPT is as follows:

```plaintext
You are an expert to classify customer statements in the sales and service domain. Based on the content of the sentence, you need to assign the most appropriate label from the following categories:
- **ordering**: When a customer **places an order**, possibly including delivery details, but always specifying **the product and quantity**.
  - Example: "Ship đến ĐH Khoa học Tự nhiên cho mình 2 Matcha Macchiato được không?" (Ship 2 Matcha Macchiatos to the University of Natural Sciences for me, please?)
[Additional categories and examples for the remaining 13 classes would be included here, omitted for brevity.]
```

For each input text in `trainfile1.csv`, ChatGPT outputs a probability distribution over the 14 classes, which is stored as `soft_labels`. These soft labels capture nuanced inter-class relationships, enhancing the distillation process.

### **Test Dataset**

The test dataset is stored in a CSV file (e.g., `testfile1.csv`) with the following columns:

- **content**: The input text (e.g., `"ship cho mình đến 82 duy tân nhé"` (Ship it to me at 82 Duy Tan, please)).
  - Type: String
  - Description: Similar to training, contains Vietnamese text to be classified.
- **label**: The class label.
  - Type: String
  - Description: Textual label (e.g., `"ordering"`, `"shipping"`, `"cancel_order"`) mapped to `label_id` using `data_label1.json`.

**Example Row**:

```
content: "Chị đặt áo hồng bé gái size 140 nhé" (Please order a pink girl’s shirt, size 140), label: ordering
content: "ship cho mình đến 82 duy tân nhé" (Ship it to me at 82 Duy Tan, please), label: shipping
```

### **Label Mapping**

Labels are mapped to integers in `data_label1.json`:

```json
{
    "ordering": 0,
    "shipping": 1,
    "cancel_order": 2,
    "deny_reject": 3,
    "agree": 4,
    "promotion": 5,
    "greetings": 6,
    "show_menu": 7,
    "thanks": 8,
    "asking PI": 9,
    "closing-open": 10,
    "thanh_toan": 11,
    "bye": 12,
    "others": 13
}
```

## **Algorithm Description**

### **Knowledge Distillation**

Knowledge distillation transfers knowledge from a large, complex teacher model to a smaller, efficient student model. In this project:

- **Teacher Model**: ChatGPT, which generates `soft_labels` in the training dataset using few-shot learning, representing probability distributions over classes.
- **Student Model**: The `uitnlp/visobert` Transformer, fine-tuned for text classification with 14 classes.
- **Process**:
  - The student model learns from both:
    - **Hard labels** (`label_id`): Ground-truth class IDs.
    - **Soft labels**: Teacher’s probability distributions, capturing richer inter-class relationships.
  - The combined loss function balances learning from hard and soft labels.

### **Combined Loss Function**

The loss function combines **Cross-Entropy (CE)** and **Kullback-Leibler (KL) Divergence** losses to train the student model effectively, leveraging both ground-truth labels and the teacher’s knowledge:

```python
loss = alpha * ce_loss + (1 - alpha) * kl_loss
```

This combined loss is critical for knowledge distillation, balancing supervised learning (via CE Loss) with learning from the teacher’s nuanced predictions (via KL Loss). Below is a detailed explanation of each component and how they work together in this project.

#### **Cross-Entropy Loss (**`ce_loss`**)**

- **Purpose**: Measures the difference between the student model’s predicted probabilities and the ground-truth hard labels (`label_id` in `trainfile1.csv`, e.g., `0` for `"ordering"`). It ensures the student correctly predicts the true class.
- ![Formula :](ce.png)
  - ( C ): Number of classes (14, as defined in `data_label1.json`).
  - ( y_i ): True label for class ( i ), where ( y_i = 1 ) if ( i ) is the correct class, and ( y_i = 0 ) otherwise (one-hot encoding).
  - ( \hat{y}_i ): Predicted probability for class ( i ), computed from the student’s logits using softmax: $ \hat{y}_i = \frac{\exp(\text{logit}i)}{\sum{j=1}^{C} \exp(\text{logit}_j)} $
- **Implementation**: In `model.py`, `nn.CrossEntropyLoss` computes CE Loss by comparing `student_logits` (model outputs) with `labels` (`batch["hard_labels"]`).
- **Role**: Encourages the student to assign high probability to the correct class. For example, for a sample with `label_id=0` (`"ordering"`), CE Loss penalizes low (\hat{y}\_0). This is essential for maintaining accuracy on the test set (`testfile1.csv`).

#### **Kullback-Leibler Divergence Loss (**`kl_loss`**)**

- **Purpose**: Measures how well the student’s output probability distribution matches the teacher’s soft labels (`soft_labels` in `trainfile1.csv`, e.g., `[0.8, 0.05, 0.03, ...]`). It transfers the teacher’s nuanced knowledge, including inter-class relationships (e.g., slight similarity between `"ordering"` and `"show_menu"`).
- **Formula**: $ \text{KL} = \sum_{i=1}^{C} p_i \log\left(\frac{p_i}{q_i}\right) $
  - ( p_i ): Teacher’s soft label probability for class ( i ).
  - ( q_i ): Student’s predicted probability for class ( i ), softened using a temperature ( T = 2.0 ): $ q_i = \frac{\exp(\text{logit}i / T)}{\sum{j=1}^{C} \exp(\text{logit}_j / T)} $
  - The student’s log-probabilities are computed as: $ \text{student_log_probs} = \text{LogSoftmax}\left(\frac{\text{student_logits}}{T}\right) $
- **Implementation**: In `model.py`, `nn.KLDivLoss` (with `reduction='batchmean'`) computes KL Loss between `student_log_probs` and `soft_labels`. The temperature ( T = 2.0 ) softens the student’s distribution to emphasize inter-class relationships.
- **Role**: Encourages the student to mimic the teacher’s distribution, improving generalization. For example, a soft label `[0.8, 0.05, 0.03, ...]` for `"ordering"` indicates secondary relevance to `"shipping"`, which KL Loss helps the student learn.

#### **Weighting Parameter (**`alpha`**)**

- **Purpose**: Balances the contributions of CE Loss and KL Loss.
- **Value**: Default `alpha = 0.05`, meaning:
  - CE Loss contributes 5% ((\alpha = 0.05)).
  - KL Loss contributes 95% ((1 - \alpha = 0.95)).
- **Impact**:
  - Low `alpha` emphasizes distillation, prioritizing the teacher’s soft labels to transfer nuanced knowledge.
  - This is suitable for the project, as `soft_labels` (generated by ChatGPT) are assumed to be high-quality, providing richer information than hard labels alone.
  - A higher `alpha` (e.g., 0.5) would focus more on hard labels, resembling standard supervised learning, but might miss the teacher’s inter-class insights.
- **Implementation**: In `model.py`, the loss is computed as:

  ```python
  loss = alpha * ce_loss + (1 - alpha) * kl_loss
  ```

#### **Why Combine CE and KL Losses?**

- **CE Loss**: Ensures the student model accurately predicts the ground-truth class, maintaining high accuracy on tasks like classifying `"đặt trà sữa"` (Place an order for milk tea) as `"ordering"`.
- **KL Loss**: Transfers the teacher’s knowledge, improving generalization by learning subtle class relationships (e.g., partial similarity between `"ordering"` and `"shipping"`).
- **Balance**: The low `alpha = 0.05` prioritizes distillation, leveraging the teacher’s soft labels to make the compact `uitnlp/visobert` model perform closer to a larger teacher model, while CE Loss keeps it grounded in the true labels \[Hinton et al., 2015\].
- **Practical Context**: In the project, the 14-class classification task benefits from this approach, as soft labels can clarify ambiguous cases (e.g., texts that might relate to multiple intents). The combined loss likely improves micro-averaged F1 scores, which you evaluate in the output Excel file (`classification_report_visobert_testfile1.xlsx`).

#### **Example in the Project**

Consider a training sample:

- **Input**: `"đặt trà sữa"` (Place an order for milk tea).
- **Hard Label**: `label_id=0` (one-hot: `[1, 0, 0, ..., 0]` for `"ordering"`).
- **Soft Label**: `[0.8, 0.05, 0.03, ..., 0.01]` (generated by ChatGPT).
- **Student Logits**: `[2.5, 0.3, 0.1, ..., 0.05]` (model output).
- **Process**:
  - **CE Loss**: Penalizes the difference between softmax(logits) and `[1, 0, 0, ...]`, pushing high probability for class 0.
  - **KL Loss**: Penalizes the difference between softened softmax(logits/2.0) and `[0.8, 0.05, 0.03, ...]`, aligning the student’s distribution with ChatGPT’s.
  - **Combined Loss**: `loss = 0.05 * ce_loss + 0.95 * kl_loss`, heavily weighted toward ChatGPT’s knowledge.

### **Implementation Details**

- **Model**: `uitnlp/visobert`, a Vietnamese-specific Transformer \[Nguyen et al., 2021\].
- **Optimizer**: AdamW with learning rate `5e-5`.
- **Scheduler**: ReduceLROnPlateau, reducing the learning rate by half if the loss plateaus for 3 epochs.
- **Training**:
  - Epochs: Configurable (default 40, tested with 1 for debugging).
  - Batch size: 16.
- **Inference**:
  - Uses hard labels from the test set for evaluation.
  - Outputs a classification report (precision, recall, F1) in an Excel file.

## **Installation**
To set up the project, follow these steps to install the required dependencies. It is recommended to use a virtual environment to avoid conflicts with system-wide packages.

1. **Prerequisites**:
   - Ensure Python 3.8 or higher is installed. You can check your Python version with:
     ```bash
     python --version
     ```
   - If Python is not installed, download it from [python.org](https://www.python.org/downloads/).

2. **Create a Virtual Environment**:
   - Navigate to your project directory:
     ```bash
     cd /path/to/Distill_with_combination_loss
     ```
   - Create a virtual environment:
     ```bash
     python -m venv venv
     ```
   - Activate the virtual environment:
     ```bash
     source venv/bin/activate  # On Linux/Mac
     venv\Scripts\activate     # On Windows
     ```

3. **Install Dependencies**:
   - Ensure `requirements.txt` is in your project directory. This file lists all required packages (e.g., `torch`, `transformers`, `pandas`).
   - Install the dependencies:
     ```bash
     pip install -r requirements.txt
     ```

4. **Verify Installation**:
   - Check that the packages are installed correctly:
     ```bash
     pip list
     ```
   - Confirm that `torch`, `transformers`, `pandas`, `openpyxl`, `scikit-learn`, `numpy`, `tqdm`, and `sentencepiece` are listed with versions meeting the requirements (e.g., `torch>=1.9.0`, `transformers>=4.20.0`).
   - Test the setup by running:
     ```bash
     python -c "import torch; import transformers; print('Setup successful')"
     ```
     If no errors occur, the environment is ready.

5. **Optional: GPU Support**:
   - If you have a compatible GPU, install the CUDA-enabled version of PyTorch for faster training. Visit [pytorch.org](https://pytorch.org/get-started/locally/) to find the appropriate `pip` command for your CUDA version (e.g., `pip install torch>=1.9.0+cu111` for CUDA 11.1).
   - Verify GPU availability:
     ```bash
     python -c "import torch; print(torch.cuda.is_available())"
     ```
     If it outputs `True`, GPU support is enabled.

**Note**: If you encounter issues (e.g., version conflicts), ensure `pip` is up-to-date (`pip install --upgrade pip`) or consult the troubleshooting section in the repository’s documentation.


## **Usage**

1. Prepare training (`trainfile1.csv`) and test (`testfile1.csv`) datasets.
2. Ensure `data_label1.json` maps all labels.
3. Run the training and testing script:

   ```bash
   python main.py
   ```
4. Check the output classification report in `combine_/classification_report_visobert_testfile1.xlsx`.

For issues or contributions, please open an issue or pull request on this repository.

## **References**

1. Hinton, G., Vinyals, O., & Dean, J. (2015). **Distilling the Knowledge in a Neural Network**. arXiv:1503.02531.
   - Introduces knowledge distillation and the combined loss `loss = alpha * ce_loss + (1 - alpha) * kl_loss` used in this project.
2. Gou, J., Yu, B., Maybank, S. J., & Tao, D. (2021). **Knowledge Distillation: A Survey**. International Journal of Computer Vision, 129(6), 1789-1819.
   - Reviews knowledge distillation techniques, including the use of the combined CE and KL loss in NLP and other domains, relevant to this project’s approach.
3. Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). **DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter**. arXiv:1910.01108.
   - Describes DistilBERT, which uses a similar combined loss for distilling BERT models, applicable to the `uitnlp/visobert` student model in this project.
4. Sun, S., Cheng, Y., Gan, Z., & Liu, J. (2019). **Patient Knowledge Distillation for BERT Model Compression**. arXiv:1908.09355.
   - Proposes a distillation method for BERT using the combined CE and KL loss, specifically for text classification tasks, aligning with this project’s objectives.
5. Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**. arXiv:1810.04805.
   - Basis for `uitnlp/visobert`, a BERT-based model for Vietnamese.
6. Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). **Attention is All You Need**. arXiv:1706.03762.
   - Describes the Transformer architecture used in `visobert`.
7. Hugging Face Transformers Documentation. **AutoModelForSequenceClassification**.
   - https://huggingface.co/docs/transformers/model_doc/auto
   - Details the implementation of `nn.CrossEntropyLoss` and model setup used in this project.
8. Nguyen, D. Q., Nguyen, A. T., & Nguyen, P. T. (2021). **ViSoBERT: A Pre-trained Language Model for Vietnamese**.
   - Describes `uitnlp/visobert`, the student model in this project.
